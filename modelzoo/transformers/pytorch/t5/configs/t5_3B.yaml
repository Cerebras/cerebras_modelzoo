train_input:
    data_processor : "T5DynamicDataProcessor"
    src_vocab_file: "./t5/c4/vocab.txt" # https://huggingface.co/t5-small/resolve/main/spiece.model.
    src_data_dir: "./t5/c4/en/train.tok.sentencepiece.3200"
    extra_ids: 100 # A number of extra ids added to the end of the vocabulary for use as sentinels.
    src_max_sequence_length: 512
    tgt_max_sequence_length: 114
    shuffle: True
    shuffle_seed: 1
    shuffle_buffer: 16384 # large buffer size allows batches to contain samples from multiple documents
    batch_size: 770 # this is more than 2**16 tokens per batch (ref batch size = 216) for perf reasons on CS system
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True

eval_input:
    data_processor : "T5DynamicDataProcessor"
    src_vocab_file: "./t5/c4/vocab.txt" # https://huggingface.co/t5-small/resolve/main/spiece.model.
    src_data_dir: "./t5/c4/en/validation.tok.sentencepiece.3200"
    extra_ids: 100 # A number of extra ids added to the end of the vocabulary for use as sentinels.
    src_max_sequence_length: 512
    tgt_max_sequence_length: 114
    shuffle: False
    shuffle_seed: 1 # also for deterministic masking
    batch_size: 770
    num_workers: 8
    prefetch_factor: 10
    persistent_workers: True

model:
    src_vocab_size: 32001
    extra_ids: 100

    ## Encoder
    encoder_num_hidden_layers: 24
    dropout_rate: 0.1

    # Encoder -- Attention
    d_kv: 128 # Size of the key, query, value projections per attention head.
    num_heads: 32 # d_kv * num_heads = hidden size.
    use_projection_bias_in_attention: False

    # Position Embeddings
    position_embedding_type: "relative"

    # Shared Weighed Embeddings
    share_embedding_weights: True
    share_encoder_decoder_embedding: True

    # T5 layer norm (no mean subtraction, and bias correction)
    use_t5_layer_norm: False

    # Encoder -- ffn
    d_ff: 16384 # Size of the intermediate feed forward layer in t5 blocks.
    d_model: 1024  # Size of the encoder layers and the pooler layer.
    encoder_nonlinearity: "relu" # {"gelu", "relu", "gated-gelu"}
    decoder_nonlinearity: "relu" # {"gelu", "relu", "gated-gelu"}
    layer_norm_epsilon: 1.0e-5

    ## Decoder
    decoder_num_hidden_layers: 24

    # Loss scaling weight, 1/{average_number_valid_tokens}
    lm_loss_weight: 0.015

    # Cerebras configs.
    mixed_precision: True
    use_bfloat16: True

optimizer:
    optimizer_type: "AdaFactor"
    disable_lr_steps_reset: True
    learning_rate:
    - scheduler: "InverseSquareRootDecay"
      warmup_steps: 2805
      scale: 1.0
    loss_scaling_factor: 1

runconfig:
    max_steps: 147072 # (524288 * 216 / 770)
    log_steps: 100
    checkpoint_steps: 10000
    eval_steps: 703
    seed: 1
    model_dir: "./model_dir"
    show_debug_metrics: False
    save_losses: True
