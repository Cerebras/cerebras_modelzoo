train_input: &base_train_input
    data_processor: "ImageNet1KProcessor"  # num_images: ~1,256,144
    # data_processor: "ViTClassificationSyntheticDataProcessor"  # num_images: ~1,256,144
    data_dir: "./computer_vision/datasets/imagenet/imagenet1k_ilsvrc2012"
    num_classes: 1000
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 2850
    micro_batch_size: 2850
    image_size: [224, 224] # [H, W]
    patch_size: [16, 16]
    image_channels: 3
    max_sequence_length: 197 # ( 224 / 16 ) * ( 224 / 16 ) + 1 (cls token)
    shuffle: True
    shuffle_seed: 42
    split: "train"

    # Ref: https://github.com/google-research/vision_transformer/blob/16fc24d2734f34b0a7b16212a4386c41fe662cb4/vit_jax/input_pipeline.py#L195
    transforms:
        # Random crop and resize
        - name: "resize"
          size: [256, 256] # [H, W]
        - name: "random_resized_crop"
          size: [224, 224] # [H, W]
          scale: [0.08, 1.0]
          ratio: [0.75, 1.33]
          interpolation: "bilinear"
        - name: "random_horizontal_flip"
          p: 0.5

        # Image normalization
        - name: "to_tensor"
        - name: "normalize"
          mean: [0.5, 0.5, 0.5]
          std: [0.5, 0.5, 0.5]

    num_workers: 2
    prefetch_factor: 2
    persistent_workers: True
    use_worker_cache: True

# TODO: check eval params
eval_input: &base_eval_input
    <<: *base_train_input
    image_size: [224, 224] # [H, W];  num_images: 50000
    # The effective batch size, which is evenly divided across "num_csx" systems used for the run
    batch_size: 2850
    shuffle: False
    transforms:
        - name: "resize"
          size: [224, 224] # [H, W]
        - name: "to_tensor"
        - name: "normalize"
          mean: [0.5, 0.5, 0.5]
          std: [0.5, 0.5, 0.5]
    split: "val"

model: &base_model
    num_classes: 1000
    # Embedding
    position_embedding_type: "learned"
    embedding_dropout_rate: 0.1
    hidden_size: 768
    # Encoder
    num_hidden_layers: 12
    layer_norm_epsilon: 1.0e-6
    # Encoder Attn
    num_heads: 12
    attention_type: "scaled_dot_product"
    attention_softmax_fp32: True
    dropout_rate: 0.1
    nonlinearity: "gelu"
    pooler_nonlinearity: "tanh"
    attention_dropout_rate: 0.1
    use_projection_bias_in_attention: True
    use_ffn_bias_in_attention: True
    # Encoder ffn
    filter_size: 3072
    use_ffn_bias: True
    # Task-specific
    initializer_range: 0.02
    norm_first: True
    # vision related params
    image_size: [224, 224]
    num_channels: 3
    patch_size: [16, 16]
    use_conv_patchified_embedding: True
    use_encoder_pooler_layer: False
    prepend_cls_token: True

    mixed_precision: True
    fp16_type: "cbfloat16"

optimizer: &base_optimizer
    optimizer_type: "Adam"
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.0003
    max_gradient_norm: 1.0
    correct_bias: True
    learning_rate:
        - scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 5.0e-4
          total_iters: 14371
        - scheduler: "CosineDecay"
          initial_learning_rate: 5.0e-4
          end_learning_rate: 0.0
          total_iters: 117854
    loss_scaling_factor: "dynamic"

runconfig: &base_runconfig
    max_steps: 132225 # 300 epoch * num_images: ~1,256,144 / bs 2850 ~= 132225
    log_steps: 1
    checkpoint_steps: 1000
    seed: 1
    eval_frequency: 1000
    num_workers_per_csx: 2
