# ESM-2 Model fine-tuning for classification tasks

## Model Overview

This directory contains implementation of the [ESM-2](https://www.science.org/doi/abs/10.1126/science.ade2574) fine-tuning model for protein sequence classification task.

A ESM-2 model instance is initialized with the weights generated by pre-training the model in a self-supervised fashion on the BERT masked language modeling (MLM) task. During fine-tuning, the model architecture is adjusted by replacing MLM output head with a classification head. The new output head is constructed by using two additional layers:
* A BERT pooling layer that maps the `[CLS]` representation of a sequence to a hidden state.
* A dense layer that maps the hidden state from pooling layer to classification logits.

## Code Structure

- `configs/`: YAML hyper-parameters configuration files.
- `model.py`: Model wrapper that inherits [Esm2ForPreTrainingModel](../model.py) class.
- `esm2_classification_models.py`: The model implementation that finetunes a [Esm2PretrainModel](../esm2_pretrain_models.py) as the base model.
- `deeploc2_data.py`: The train/test dataset generation tool.

## Dataset Generation

### Data Preparation

We will create our training and test data using a portion of the [DeepLoc-2 dataset](https://services.healthtech.dtu.dk/services/DeepLoc-2.0/), which includes several thousand SwissProt proteins with experimentally determined locations. We will retain high-quality sequences ranging from 100 to 1026 amino acids.

To generate the training and test data, simply run the following command:
```
python src/models/src/cerebras/modelzoo/models/nlp/esm2/classification/deeploc2_data.py --output_train_csv_path /path/to/train.csv --output_test_csv_path /path/to/test.csv
```

The script will download the DeepLoc-2 dataset, tokenize it and properly format it into the desired CSV format for model consumption. The tokenizer that is being used is `facebook/esm2_t33_650M_UR50D`.

### Features

The output CSV data has the following feature columns:

- `input_ids`: Input token IDs, padded with `0`s to `max_sequence_length`. The tokens in the dataset are mapped to these IDs using the vocabulary file. These values should be between `0` and `vocab_size - 1`, inclusive. The first token should be the special `[CLS]` token. The end of each sentence should be marked by the special `[SEP]` token. So, a sentence pair should be separated by additional `[SEP]` token.

- `attention_mask`: Mask for padded positions. Has values `0` on the padded positions and `1` elsewhere.

- `label`: A label is an integer per input sequence corresponding to its classification labels.

## Run fine-tuning

**Parameter settings in YAML config file**: The config YAML files are located in the [configs](configs/) directory. Before starting a pre-training run, make sure that in the YAML config file you are using:

- The `train_input.data_dir` parameter points to the directory of the dataset.
- The `train_input.max_sequence_length` parameter corresponds to the sequence length of the dataset. For this task, it is _1026_.
- The `train_input.batch_size` parameter will set the batch size for the training.

Same applies for the `eval_input`.

Additionally, we define the fine-tuning related parameters:

- The `runconfig.load_checkpoint_states` specifies the state dict key to load from a checkpoint for fine-tuning. It is set to `"model"` by default.
- The `runconfig.disable_strict_checkpoint_loading` is set to enable partial loading of a model.

## To compile/validate, run train and eval on Cerebras System

Please follow the instructions on our [quickstart in the Developer Docs](https://docs.cerebras.net/en/latest/wsc/getting-started/cs-appliance.html).

> **Note**: To specify an ESM-2 pretrained checkpoint file:
* `--checkpoint_path` is the path to the saved checkpoint from ESM-2 pre-training. It can be passed in the command line arguments. Please follow [this guide](https://docs.cerebras.net/en/latest/wsc/port/porting-checkpoints.html) on converting a PyTorch model checkpoint file to Cerebras format.

## To run train and eval on GPU/CPU

If running on a cpu or gpu, activate the environment from [Python GPU Environment setup](../../../../../../../PYTHON-SETUP.md), and simply run:

```
python run.py CPU --mode train --params /path/to/yaml --model_dir /path/to/model_dir --checkpoint_path /path/to/ckpt
```
or
```
python run.py GPU --mode train --params /path/to/yaml --model_dir /path/to/model_dir --checkpoint_path /path/to/ckpt
```

> **Note**: Change the command to `--mode eval` for evaluation.

## Configs included for this model
Below is the list of yaml config files included for this model implementation at [configs](./configs/) folder:

- `params_esm2_650M_UR50_classifier.yaml` have the standard ESM-2 config with `hidden_size=1280, num_hidden_layers=33, num_heads=20` as a backbone. The pretrained PyTorch model checkpoint is located [here](https://huggingface.co/facebook/esm2_t33_650M_UR50D/blob/main/pytorch_model.bin).
- `params_esm2_a10_UR50_classifier.yaml` have the standard ESM-2 config with `hidden_size=640, num_hidden_layers=30, num_heads=20` as a backbone. The pretrained PyTorch model checkpoint is located [here](https://huggingface.co/facebook/esm2_t30_150M_UR50D/blob/main/pytorch_model.bin).

Choose the appropriate configuration based on your computational resources and desired model size. Feel free to add and experiment with your own configuration as well.