setup:
    data:
        source: "/path/to/input" # replace with your directory
        type: "local"

    output_dir: "/path/to/output" # replace with your directory
    processes: 1
    mode: "finetuning"

processing:
    custom_tokenizer: "gpt2tokenizer"
    tokenizer_params:
        vocab_file: "/path/to/vocab"
        encoder_file: "/path/to/encoder"

    max_seq_length: 2048
    short_seq_prob: 0.0

    write_in_batch: True

    resume_from_checkpoint: False
    seed: 0

    read_hook: "cerebras.modelzoo.data_preparation.data_preprocessing.hooks:prompt_completion_key"
    read_hook_kwargs:
        data_keys:
            prompt_key: "article_content"
            completion_key: "summary"

dataset:
    use_ftfy: True
    ftfy_normalizer: "NFC"
    wikitext_detokenize: False

    sep_token: "<SEP>"